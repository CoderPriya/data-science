@article{TACL452,
	author = {Subhro Roy and Tim Vieira and Dan Roth},
	title = {Reasoning about Quantities in Natural Language},
	journal = {Transactions of the Association for Computational Linguistics},
	volume = {3},
	year = {2015},
	keywords = {},
	abstract = {Little work from the Natural Language Processing community has targeted the role of quantities in Natural Language Understanding. This paper takes some key steps towards facilitating reasoning about quantities expressed in natural language. We investigate two different tasks of numerical reasoning. First, we consider Quantity Entailment, a new task formulated to understand the role of quantities in general textual inference tasks. Second, we consider the problem of automatically understanding and solving elementary school math word problems. In order to address these quantitative reasoning problems we first develop a computational approach which we show to successfully recognize and normalize textual expressions of quantities. We then use these capabilities to further develop algorithms to assist reasoning in the context of the aforementioned tasks.},
	issn = {2307-387X},
	url = {https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/452},
	pages = {1--13}
}
@article{TACL522,
	author = {Sourav Dutta and Gerhard Weikum},
	title = {Cross-Document Co-Reference Resolution using Sample-Based Clustering with Knowledge Enrichment},
	journal = {Transactions of the Association for Computational Linguistics},
	volume = {3},
	year = {2015},
	keywords = {},
	abstract = {Identifying and linking named entities across information sources is the basis of knowledge acquisition and at the heart of Web search, recommendations, and analytics. An important problem in this context is cross-document co-reference resolution (CCR): computing equivalence classes of textual mentions denoting the same entity, within and across documents. Prior methods employ ranking, clustering, or probabilistic graphical models using syntactic features and distant features from knowledge bases. However, these methods exhibit limitations regarding run-time and robustness.
   This paper presents the CROCS framework for unsupervised CCR, improving the state of the art in two ways. First, we extend the way knowledge bases are harnessed, by constructing a notion of semantic summaries for intra-document co-reference chains using co-occurring entity mentions belonging to different chains. Second, we reduce the computational cost by a new algorithm that embeds sample-based bisection, using spectral clustering or graph partitioning, in a hierarchical clustering process. This allows scaling up CCR to large corpora. Experiments with three datasets show significant gains in output quality, compared to the best prior methods, and the run-time efficiency of CROCS.
  },
	issn = {2307-387X},
	url = {https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/522},
	pages = {15--28}
}
@article{TACL465,
	author = {Oscar Täckström and Kuzman Ganchev and Dipanjan Das},
	title = {Efficient Inference and Structured Learning for Semantic Role Labeling},
	journal = {Transactions of the Association for Computational Linguistics},
	volume = {3},
	year = {2015},
	keywords = {},
	abstract = {We present a dynamic programming algorithm for efficient constrained inference in semantic role labeling. The algorithm tractably captures a majority of the structural constraints examined by prior work in this area, which has resorted to either approximate methods or off-the-shelf integer linear programming solvers. In addition, it allows training a globally-normalized log-linear model with respect to constrained conditional likelihood. We show that the dynamic program is several times faster than an off-the-shelf integer linear programming solver, while reaching the same solution. Furthermore, we show that our structured model results in significant improvements over its local counterpart, achieving state-of-the-art results on both PropBank- and FrameNet-annotated corpora. },
	issn = {2307-387X},
	url = {https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/465},
	pages = {29--41}
}
@article{TACL403,
	author = {Michael Paul and Mark Dredze},
	title = {SPRITE: Generalizing Topic Models with Structured Priors},
	journal = {Transactions of the Association for Computational Linguistics},
	volume = {3},
	year = {2015},
	keywords = {},
	abstract = {We introduce SPRITE, a family of topic models that incorporates structure into model priors as a function of underlying components. The structured priors can be constrained to model topic hierarchies, factorizations, correlations, and supervision, allowing SPRITE to be tailored to particular settings. We demonstrate this flexibility by constructing a SPRITE-based model to jointly infer topic hierarchies and author perspective, which we apply to corpora of political debates and online reviews. We show that the model learns intuitive topics, outperforming several other topic models at predictive tasks.},
	issn = {2307-387X},
	url = {https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/403},
	pages = {43--57}
}
@article{TACL485,
	author = {Jing Wang and Mohit Bansal and Kevin Gimpel and Brian Ziebart and Clement Yu},
	title = {A Sense-Topic Model for Word Sense Induction with Unsupervised Data Enrichment},
	journal = {Transactions of the Association for Computational Linguistics},
	volume = {3},
	year = {2015},
	keywords = {},
	abstract = {Word sense induction (WSI) seeks to automatically discover the senses of a word in a corpus via unsupervised methods. We propose a sense-topic model for WSI, which treats sense and topic as two separate latent variables to be inferred jointly. Topics are informed by the entire document, while senses are informed by the local context surrounding the ambiguous word. We also discuss unsupervised ways of enriching the original corpus in order to improve model performance, including using neural word embeddings and external corpora to expand the context of each data instance. We demonstrate significant improvements over the previous state-of-the-art, achieving the best results reported to date on the SemEval-2013 WSI task.},
	issn = {2307-387X},
	url = {https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/485},
	pages = {59--71}
}
@article{TACL440,
	author = {Annie Louis and Mirella Lapata},
	title = {Which Step Do I Take First? Troubleshooting with Bayesian Models},
	journal = {Transactions of the Association for Computational Linguistics},
	volume = {3},
	year = {2015},
	keywords = {},
	abstract = {Online discussion forums and community question-answering websites provide one of the primary avenues for online users to share information. In this paper, we propose text mining techniques which aid users navigate troubleshooting-oriented data such as questions asked on forums and their suggested solutions. We introduce Bayesian generative models of the troubleshooting data and apply them to two interrelated tasks (a) predicting the complexity of the solutions (e.g., plugging a keyboard in the computer is easier compared to installing a special driver) and (b) presenting them in a ranked order from least to most complex. Experimental results show that our models are on par with human performance on these tasks, while outperforming baselines based on solution length or readability.   },
	issn = {2307-387X},
	url = {https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/440},
	pages = {73--85}
}


@article{TACL555,
	author = {Hua He and Jimmy Lin and Adam Lopez},
	title = {Gappy Pattern Matching on {GPUs} for On-Demand Extraction of Hierarchical Translation Grammars},
	journal = {Transactions of the Association for Computational Linguistics},
	volume = {3},
	year = {2015},
	keywords = {},
	abstract = {Grammars for machine translation can be materialized on demand by finding source phrases in an indexed parallel corpus and extracting their translations. This approach is limited in practical applications by the computational expense of online lookup and extraction. For phrase-based models, recent work has shown that on-demand grammar extraction can be greatly accelerated by parallelization on general purpose graphics processing units (GPUs), but these algorithms do not work for hierarchical models, which require matching patterns that contain gaps. We address this limitation by presenting a novel GPU algorithm for on-demand hierarchical grammar extraction that is at least an order of magnitude faster than a comparable CPU algorithm when processing large batches of sentences. In terms of end-to-end translation, with decoding on the CPU, we increase throughput by roughly two thirds on a standard MT evaluation dataset. The GPU necessary to achieve these improvements increases the cost of a server by about a third. We believe that GPU-based extraction of hierarchical grammars is an attractive proposition, particularly for MT applications that demand high throughput.},
	issn = {2307-387X},
	url = {https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/555},
	pages = {87--100}
}


@article{TACL276,
        author = {Brian McMahan and Matthew Stone},
        title = {A Bayesian Model of Grounded Color Semantics},
        journal = {Transactions of the Association for Computational Linguistics},
        volume = {3},
        year = {2015},
        keywords = {},
        abstract = {Natural language meanings allow speakers to encode important
real-world distinctions, but corpora of grounded language use also reveal
that speakers categorize the world in different ways and describe situations
with different terminology.  To learn meanings from data, we therefore need
to link underlying representations of meaning to models of speaker judgment
and speaker choice.  This paper describes a new approach to this problem: we
model variability through uncertainty in categorization boundaries and
distributions over preferred vocabulary.  We apply the approach to a large
data set of color descriptions, where statistical evaluation documents its
accuracy.  The results are available as a Lexicon of Uncertain Color
Standards (LUX), which supports future efforts in grounded language
understanding and generation by probabilistically mapping 829 English color
descriptions to potentially context-sensitive regions in HSV color space.},
        issn = {2307-387X},
        url =
{https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/276},
        pages = {103--115}
}@article{TACL476,
        author = {Congle Zhang and Stephen Soderland and Daniel Weld},
        title = {Exploiting Parallel News Streams for Unsupervised Event
Extraction},
        journal = {Transactions of the Association for Computational Linguistics},
        volume = {3},
        year = {2015},
        keywords = {},
        abstract = {Most approaches to relation extraction, the task of extracting
ground facts from natural language text, are based on machine learning and
thus starved by scarce training data. Manual annotation is too expensive to
scale to a comprehensive set of relations. Distant supervision, which
automatically creates training data, only works with relations that already
populate a knowledge base (KB). Unfortunately, KBs such as FreeBase rarely
cover event relations (e.g. “person travels to location”). Thus, the
problem of extracting a wide range of events — e.g., from news streams —
is an important, open challenge. This paper introduces NewsSpike-RE, a
novel, unsupervised algorithm that discovers event relations and then learns
to extract them. NewsSpike-RE uses a novel probabilistic graphical model to
cluster sentences describing similar events from parallel news streams.
These clusters then comprise training data for the extractor. Our evaluation
shows that NewsSpike-RE generates high quality training sentences and learns
extractors that perform much better than rival approaches, more than
doubling the area under a precision-recall curve compared to Universal
Schemas.},
        issn = {2307-387X},
        url =
{https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/476},
        pages = {117--129}
}
@article{TACL472,
        author = {Yufan Guo and Roi Reichart and Anna Korhonen},
        title = {Unsupervised Declarative Knowledge Induction for Constraint-Based
Learning of Information Structure in Scientific Documents},
        journal = {Transactions of the Association for Computational Linguistics},
        volume = {3},
        year = {2015},
        keywords = {},
        abstract = {Inferring the information structure of scientific documents is
useful for many NLP applications. Existing approaches to this task require
substantial human effort. We propose a framework for constraint learning
that reduces human involvement considerably. Our model uses topic models to
identify latent topics and their key linguistic features in input documents,
induces constraints from this information and maps sentences to their
dominant information structure categories through a constrained unsupervised
model. When the induced constraints are combined with a fully unsupervised
model, the resulting model challenges existing lightly supervised
feature-based models as well as unsupervised models that use manually
constructed declarative knowledge. Our results demonstrate that useful
declarative knowledge can be learned from data with very limited human
involvement.},
        issn = {2307-387X},
        url =
{https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/472},
        pages = {131--143}
}@article{TACL494,
        author = {Andrew Chisholm and Ben Hachey},
        title = {Entity Disambiguation with Web Links},
        journal = {Transactions of the Association for Computational Linguistics},
        volume = {3},
        year = {2015},
        keywords = {},
        abstract = {Entity disambiguation with Wikipedia relies on structured
information from redirect pages, article text, inter-article links, and
categories. We explore whether web links can replace a curated
encyclopaedia, obtaining entity prior, name, context, and coherence models
from a corpus of web pages with links to Wikipedia. Experiments compare web
link models to Wikipedia models on well-known CoNLL and TAC data sets.
Results show that using 34 million web links approaches Wikipedia
performance. Combining web link and Wikipedia models produces the best-known
disambiguation accuracy of 88.7 on standard newswire test data.},
        issn = {2307-387X},
        url =
{https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/494},
        pages = {145--156}
}@article{TACL458,
        author = {Karthik Narasimhan and Regina Barzilay and Tommi Jaakkola},
        title = {An Unsupervised Method for Uncovering Morphological Chains},
        journal = {Transactions of the Association for Computational Linguistics},
        volume = {3},
        year = {2015},
        keywords = {},
        abstract = {Most state-of-the-art systems today produce morphological
analysis based only on orthographic patterns. In contrast, we propose a
model for unsupervised morphological analysis that integrates orthographic
and semantic views of words. We model word formation in terms of
morphological chains, from base words to the observed words, breaking the
chains into parent-child relations. We use log-linear models with morpheme
and word-level features to predict possible parents, including their
modifications, for each word. The limited set of candidate parents for each
word render contrastive estimation feasible. Our model consistently matches
or outperforms five state-of-the-art systems on Arabic, English and
Turkish.},
        issn = {2307-387X},
        url =
{https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/458},
        pages = {157--167}
}
@article{TACL510,
        author = {Rico Sennrich},
        title = {Modelling and Optimizing on Syntactic N-Grams for Statistical
Machine Translation},
        journal = {Transactions of the Association for Computational Linguistics},
        volume = {3},
        year = {2015},
        keywords = {},
        abstract = {The role of language models in SMT is to promote fluent
translation
output, but traditional n-gram language models are unable to capture
fluency phenomena between distant words, such as some morphological
agreement phenomena, subcategorisation, and syntactic collocations with
string-level gaps. Syntactic language models have the potential to fill
this modelling gap. We propose a language model for dependency
structures that is relational rather than configurational and thus
particularly suited for languages with a (relatively) free word order.
It is trainable with Neural Networks, and not only improves over
standard n-gram language models, but also outperforms related syntactic
language models. We empirically demonstrate its effectiveness in terms
of perplexity and as a feature function in string-to-tree SMT from
English to German and Russian. We also show that using a syntactic
evaluation metric to tune the log-linear parameters of an SMT system
further increases translation quality when coupled with a syntactic
language model.},
        issn = {2307-387X},
        url =
{https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/510},
        pages = {169--182}
}
@article{TACL568,
        author = {Angeliki Lazaridou and Georgiana Dinu and Adam Liska and Marco
Baroni},
        title = {From Visual Attributes to Adjectives through Decompositional
Distributional Semantics},
        journal = {Transactions of the Association for Computational Linguistics},
        volume = {3},
        year = {2015},
        keywords = {},
        abstract = {As automated image analysis progresses, there is increasing
interest  in richer linguistic annotation of pictures, with attributes  of
objects (e.g., furry, brown...) attracting most  attention. By building on
the recent \"zero-shot learning\" approach, and paying attention to the
linguistic nature of  attributes as noun modifiers, and specifically
adjectives, we show  that it is possible to tag images with
attribute-denoting adjectives  even when no training data containing the
relevant annotation are  available. Our approach relies on two key
observations. First,  objects can be seen as bundles of attributes,
typically expressed as  adjectival modifiers (a dog is something furry,
brown, etc.), and thus a function trained to map visual  representations of
objects to nominal labels can implicitly learn to  map attributes to
adjectives. Second, objects and attributes come  together in pictures (the
same thing is a dog and it is  brown). We can thus achieve better attribute
(and object)  label retrieval by treating images as \"visual phrases\", and
decomposing their linguistic representation into an  attribute-denoting
adjective and an object-denoting noun. Our  approach performs comparably to
a method exploiting manual attribute  annotation, it outperforms various
competitive alternatives in both  attribute and object annotation, and it
automatically constructs  attribute-centric representations that
significantly improve  performance in supervised object recognition.},
        issn = {2307-387X},
        url =
{https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/568},
        pages = {183--196}
}
{TACL550,
        author = {Daniel Fried and Peter Jansen and Gustave Hahn-Powell and Mihai
Surdeanu and Peter Clark},
        title = {Higher-order Lexical Semantic Models for Non-factoid Answer
Reranking},
        journal = {Transactions of the Association for Computational Linguistics},
        volume = {3},
        year = {2015},
        keywords = {},
        abstract = {Lexical semantic models provide robust performance for question
answering, but, in general, can only capitalize on direct evidence seen
during training. For example, monolingual alignment models acquire term
alignment probabilities from semi-structured data such as question-answer
pairs; neural network language models learn term embeddings from
unstructured text. All this knowledge is then used to estimate the semantic
similarity between question and answer candidates.  We introduce a
higher-order formalism that allows all these lexical semantic models to
chain direct evidence to construct indirect associations between question
and answer texts, by casting the task as the traversal of graphs that encode
direct term associations.  Using a corpus of 10,000 questions from Yahoo!
Answers, we experimentally demonstrate that higher-order methods are broadly
applicable to alignment and language models, across both word and syntactic
representations. We show that an important criterion for success is
controlling for the semantic drift that accumulates during graph traversal.
All in all, the proposed higher-order approach improves five out of the six
lexical semantic models investigated, with relative gains of up to +13\%
over their first-order variants. },
        issn = {2307-387X},
        url =
{https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/550},
        pages = {197--210}
}
@article{TACL570,
	author = {Omer Levy and Yoav Goldberg and Ido Dagan},
	title = {Improving Distributional Similarity with Lessons Learned from Word Embeddings},
	journal = {Transactions of the Association for Computational Linguistics},
	volume = {3},
	year = {2015},
	keywords = {},
	abstract = {Recent trends suggest that neural-network-inspired word embedding models outperform traditional count-based distributional models on word similarity and analogy detection tasks. We reveal that much of the performance gains of word embeddings are due to certain system design choices and hyperparameter optimizations, rather than the embedding algorithms themselves. Furthermore, we show that these modifications can be transferred to traditional distributional models, yielding similar gains. In contrast to prior reports, we observe mostly local or insignificant performance differences between the methods, with no global advantage to any single approach over the others.},
	issn = {2307-387X},
	url = {https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/570},
	pages = {211--225}
}
@article{TACL586,
        author = {Mo Yu and Mark Dredze},
        title = {Learning Composition Models for Phrase Embeddings},
        journal = {Transactions of the Association for Computational Linguistics},
        volume = {3},
        year = {2015},
        keywords = {},
        abstract = {Lexical embeddings can serve as useful representations for
words for a variety of NLP tasks, but learning embeddings for phrases can be
challenging. While separate embeddings are learned for each word, this is
infeasible for every phrase. We construct phrase embeddings by learning how
to compose word embeddings using features that capture phrase structure and
context. We propose efficient unsupervised and task-specific learning
objectives that scale our model to large datasets. We demonstrate
improvements on both language modeling and several phrase semantic
similarity tasks with various phrase lengths. We make the implementation of
our model and the datasets available for general use.},
        issn = {2307-387X},
        url =
{https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/586},
        pages = {227--242}
}
@article{TACL564,
	author = {Maha Althobaiti and Udo Kruschwitz and Massimo Poesio},
	title = {Combining Minimally-supervised Methods for Arabic Named Entity Recognition},
	journal = {Transactions of the Association for Computational Linguistics},
	volume = {3},
	year = {2015},
	keywords = {},
	abstract = {Supervised methods can achieve high performance on NLP tasks, such as Named Entity Recognition (NER), but new annotations are required for every new domain and/or genre change. This has motivated research in minimally supervised methods such as semi-supervised learning and distant learning, but neither technique has yet achieved performance levels comparable to those of supervised methods. Semi-supervised methods tend to have very high precision but comparatively low recall, whereas distant learning tends to achieve higher recall but lower precision. This complementarity suggests that better results may be obtained by combining the two types of minimally supervised methods. In this paper we present a novel approach to Arabic NER using a combination of semi-supervised and distant learning techniques. We trained a semi-supervised NER classifier and another one using distant learning techniques, and then combined them using a variety of classifier combination schemes, including the Bayesian Classifier Combination (BCC) procedure recently proposed for sentiment analysis. According to our results, the BCC model leads to an increase in performance of 8 percentage points over the best base classifiers.},
	issn = {2307-387X},
	url = {https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/564},
	pages = {243--255}
}
@article{TACL548,
	author = {Jayant Krishnamurthy and Tom Mitchell},
	title = {Learning a Compositional Semantics for Freebase with an Open Predicate Vocabulary},
	journal = {Transactions of the Association for Computational Linguistics},
	volume = {3},
	year = {2015},
	keywords = {},
	abstract = {We present an approach to learning a model-theoretic semantics for
natural language tied to Freebase. Crucially, our approach uses an
open predicate vocabulary, enabling it to produce denotations for
phrases such as \"Republican front-runner from Texas\" whose semantics
cannot be represented using the Freebase schema. Our approach directly
converts a sentence's syntactic CCG parse into a logical form
containing predicates derived from the words in the sentence,
assigning each word a consistent semantics across sentences. This
logical form is evaluated against a learned probabilistic database
that defines a distribution over denotations for each textual
predicate. A training phase produces this probabilistic database using
a corpus of entity-linked text and probabilistic matrix factorization
with a novel ranking objective function. We evaluate our approach on a
compositional question answering task where it outperforms several
competitive baselines. We also compare our approach against manually
annotated Freebase queries, finding that our open predicate vocabulary
enables us to answer many questions that Freebase cannot.},
	issn = {2307-387X},
	url = {https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/548},
	pages = {257--270}
}
@article{TACL594,
	author = {Haitong Yang and Tao Zhuang and Chengqing Zong},
	title = {Domain Adaptation for Syntactic and Semantic Dependency Parsing Using Deep Belief Networks},
	journal = {Transactions of the Association for Computational Linguistics},
	volume = {3},
	year = {2015},
	keywords = {},
	abstract = {The abstract:   In current systems for syntactic and semantic dependency parsing, people usually define a very high-dimensional feature space to achieve good performance. But these systems often suffer severe performance drops on out-of-domain test data due to the diversity of features of different domains. This paper focuses on how to relieve this domain adaptation problem with the help of unlabeled target domain data. We propose a deep learning method to adapt both syntactic and semantic parsers. With additional unlabeled target domain data, our method can learn a latent feature representation (LFR) that is beneficial to both domains. Experiments on English data in the CoNLL 2009 shared task show that our method largely reduced the performance drop on out-of-domain test data. Moreover, we get a Macro F1 score that is 2.32 points higher than the best system in the CoNLL 2009 shared task in out-of-domain tests.},
	issn = {2307-387X},
	url = {https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/594},
	pages = {271--282}
}
@article{TACL549,
        author = {Wei Xu and Chris Callison-Burch and Courtney Napoles},
        title = {Problems in Current Text Simplification Research: New Data Can
Help},
        journal = {Transactions of the Association for Computational Linguistics},
        volume = {3},
        year = {2015},
        keywords = {},
        abstract = {Simple Wikipedia has dominated simplification research in the
past 5 years. In this opinion paper, we argue that focusing on Wikipedia
limits simplification research. We back up our arguments with corpus
analysis and by highlighting statements that other researchers have made in
the simplification literature. We introduce a new simplification dataset
that is a significant improvement over Simple Wikipedia, and present a novel
quantitative-comparative approach to study the quality of simplification
data resources. },
        issn = {2307-387X},
        url =
{https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/549},
        pages = {283--297}
}
@article{TACL582,
        author = {Dat Quoc Nguyen and Richard Billingsley and Lan Du and Mark
Johnson},
        title = {Improving Topic Models with Latent Feature Word Representations},
        journal = {Transactions of the Association for Computational Linguistics},
        volume = {3},
        year = {2015},
        keywords = {},
        abstract = {Probabilistic topic models are widely used to discover latent
topics in document collections, while latent feature vector representations
of words have been used to obtain high performance in many NLP tasks. In
this paper, we extend two different Dirichlet multinomial topic models by
incorporating latent feature vector representations of words trained on very
large corpora to improve the word-topic mapping learnt on a smaller corpus.
Experimental results show that by using information from the external
corpora, our new models produce significant improvements on topic coherence,
document clustering and document classification tasks, especially on
datasets with few or short documents.},
        issn = {2307-387X},
        url =
{https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/582},
        pages = {299--313}
}
@article{TACL528,
	author = {Xiao Ling and Sameer Singh and Daniel Weld},
	title = {Design Challenges for Entity Linking},
	journal = {Transactions of the Association for Computational Linguistics},
	volume = {3},
	year = {2015},
	keywords = {},
	abstract = {Recent research on entity linking (EL) has introduced a
plethora of promising techniques, ranging from deep neural networks to joint
inference. But despite numerous papers there is surprisingly little
understanding of the state of the art in EL. We attack this confusion by
analyzing differences between several versions of the EL problem and
presenting a simple yet effective, modular, unsupervised system, called
Vinculum, for entity linking. We conduct an extensive evaluation on nine
data sets, comparing Vinculum with two state-of-the-art systems, and
elucidate key aspects of the system that include mention extraction,
candidate generation, entity type prediction, entity coreference, and
coherence.},
	issn = {2307-387X},
	url =
{https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/528},
	pages = {315--328}
}
@article{TACL536,
        author = {Yangfeng Ji and Jacob Eisenstein},
        title = {One Vector is Not Enough: Entity-Augmented Distributed Semantics
for Discourse Relations},
        journal = {Transactions of the Association for Computational Linguistics},
        volume = {3},
        year = {2015},
        keywords = {},
        abstract = {Discourse relations bind smaller linguistic units into coherent
texts. Automatically identifying discourse relations is difficult, because
it requires understanding the semantics of the linked arguments. A more
subtle challenge is that it is not enough to represent the meaning of each
argument of a discourse relation, because the relation may depend on links
between lower-level components, such as entity mentions. Our solution
computes distributed meaning representations for each discourse argument by
composition up the syntactic parse tree. We also perform a downward
compositional pass to capture the meaning of coreferent entity mentions.
Implicit discourse relations are then predicted from these two
representations, obtaining substantial improvements on the Penn Discourse
Treebank.},
        issn = {2307-387X},
        url =
{https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/536},
        pages = {329--344}
}
@article{TACL571,
        author = {John Wieting and Mohit Bansal and Kevin Gimpel and Karen
Livescu},
        title = {From Paraphrase Database to Compositional Paraphrase Model and
Back},
        journal = {Transactions of the Association for Computational Linguistics},
        volume = {3},
        year = {2015},
        keywords = {},
        abstract = {The Paraphrase Database (PPDB; Ganitkevitch et al., 2013) is an
extensive semantic resource, consisting of a list of phrase pairs with
(heuristic) confidence estimates. However, it is still unclear how it can
best be used, due to the heuristic nature of the confidences and its
necessarily incomplete coverage. We propose models to leverage the phrase
pairs from the PPDB to build parametric paraphrase models that score
paraphrase pairs more accurately than the PPDB’s internal scores while
simultaneously improving its coverage. They allow for learning phrase
embeddings as well as improved word embeddings. Moreover, we introduce two
new, manually annotated datasets to evaluate short-phrase paraphrasing
models. Using our paraphrase model trained using PPDB, we achieve
state-of-the-art results on standard word and bigram similarity tasks and
beat strong baselines on our new short phrase paraphrase tasks.},
        issn = {2307-387X},
        url =
{https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/571},
        pages = {345--358}
}
@article{TACL631,
	author = {Wolfgang Seeker and Özlem Çetinoğlu},
	title = {A Graph-based Lattice Dependency Parser for Joint Morphological Segmentation and Syntactic Analysis},
	journal = {Transactions of the Association for Computational Linguistics},
	volume = {3},
	year = {2015},
	keywords = {},
	abstract = {Space-delimited words in Turkish and Hebrew text can be further segmented into meaningful units, but syntactic and semantic context is necessary to predict segmentation. At the same time, predicting correct syntactic structures relies on correct segmentation. We present a graph-based lattice dependency parser that operates on morphological lattices to represent different segmentations and morphological analyses for a given input sentence. The lattice parser predicts a dependency tree over a path in the lattice and thus solves the joint task of segmentation, morphological analysis, and syntactic parsing. We conduct experiments on the Turkish and the Hebrew treebank and show that the joint model outperforms three state-of-the-art pipeline systems on both data sets. Our work corroborates findings from constituency lattice parsing for Hebrew and presents the first results for full lattice parsing on Turkish.},
	issn = {2307-387X},
	url = {https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/631},
	pages = {359--373}
}
@article{TACL616,
	author = {Germán Kruszewski and Denis Paperno and Marco Baroni},
	title = {Deriving Boolean Structures from Distributional Vectors},
	journal = {Transactions of the Association for Computational Linguistics},
	volume = {3},
	year = {2015},
	keywords = {},
	abstract = {Corpus-based distributional semantic models capture degrees of semantic relatedness among the words of very large vocabularies, but have problems with logical phenomena such as entailment, that are instead elegantly handled by model-theoretic approaches, which, in turn, do not scale up.We combine the advantages of the two views by inducing a mapping from distributional vectors of words (or sentences) into a Boolean structure of the kind in which natural language terms are assumed to denote. We evaluate this Boolean Distributional Semantic Model (BDSM) on recognizing entailment between words and sentences. The method achieves results comparable to a state-of-the art SVM, degrades more gracefully when less training data are available and displays interesting qualitative properties.},
	issn = {2307-387X},
	url = {https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/616},
	pages = {375--388}
}

@article{TACL520,
        author = {Chia-ying Lee and Timothy O'Donnell and James Glass},
        title = {Unsupervised Lexicon Discovery from Acoustic Input},
        journal = {Transactions of the Association for Computational Linguistics},
        volume = {3},
        year = {2015},
        keywords = {},
        abstract = {We present a model of unsupervised phonological lexicon
discovery -- the problem of simultaneously learning phoneme-like and
word-like units from acoustic input. Our model builds on earlier models of
unsupervised phone-like unit discovery from acoustic data (Lee and Glass,
2012), and unsupervised symbolic lexicon discovery using the Adaptor Grammar
framework (Johnson et al., 2006), integrating these earlier approaches using
a probabilistic model of phonological variation. We show that the model is
competitive with state-of-the-art spoken term discovery systems, and present
analyses exploring the model's behavior and the kinds of linguistic
structures it learns.       },
        issn = {2307-387X},
        url =
{https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/520},
        pages = {389--403}
}
@article{TACL604,
        author = {Sebastian Martschat and Michael Strube},
        title = {Latent Structures for Coreference Resolution},
        journal = {Transactions of the Association for Computational Linguistics},
        volume = {3},
        year = {2015},
        keywords = {},
        abstract = {Machine learning approaches to coreference resolution vary
greatly in the modeling of the problem: while early approaches operated on
the mention pair level, current research focuses on ranking architectures
and antecedent trees. We propose a unified representation of different
approaches to coreference resolution in terms of the structure they operate
on. We represent several coreference resolution approaches proposed in the
literature in our framework and evaluate their performance. Finally, we
conduct a systematic analysis of the output of these approaches,
highlighting differences and similarities.},
        issn = {2307-387X},
        url =
{https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/604},
        pages = {405--418}
}
@article{TACL618,
	author = {Ella Rabinovich and Shuly Wintner},
	title = {Unsupervised Identiﬁcation of Translationese},
	journal = {Transactions of the Association for Computational Linguistics},
	volume = {3},
	year = {2015},
	keywords = {},
	abstract = {Translated texts are distinctively different from original ones, to the extent that supervised text classification methods can distinguish between them with high accuracy. These differences were proven useful for statistical machine translation. However, it has been suggested that the accuracy of translation detection deteriorates when the classifier is evaluated outside the domain it was trained on.We show that this is indeed the case, in a variety of evaluation scenarios. We then show that unsupervised classification is highly accurate on this task. We suggest a method for determining the correct labels of the clustering outcomes, and then use the labels for voting, improving the accuracy even further. Moreover, we suggest a simple method for clustering in the challenging case of mixed-domain datasets, in spite of the dominance of domain-related features over translation-related ones. The result is an effective, fully-unsupervised method for distinguishing between original and translated texts that can be applied to new domains with reasonable accuracy. },
	issn = {2307-387X},
	url = {https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/618},
	pages = {419--432}
}
@article{TACL480,
	author = {Ryan Cotterell and Nanyun Peng and Jason Eisner},
	title = {Modeling Word Forms Using Latent Underlying Morphs and Phonology},
	journal = {Transactions of the Association for Computational Linguistics},
	volume = {3},
	year = {2015},
	keywords = {},
	abstract = {The observed pronunciations or spellings of words are often explained as arising from the \"underlying forms\" of their morphemes.  These forms are latent strings that linguists try to reconstruct by hand.  We propose to reconstruct them automatically at scale, enabling generalization to new words.  Given some surface word types of a concatenative language along with the abstract morpheme sequences that they express, we show how to recover consistent underlying forms for these morphemes, together with the (stochastic) phonology that maps each concatenation of underlying forms to a surface form.  Our technique involves loopy belief propagation in a natural directed graphical model whose variables are unknown strings and whose conditional distributions are encoded as finite-state machines with trainable weights.  We define training and evaluation paradigms for the task of surface word prediction, and report results on subsets of 7 languages.},
	issn = {2307-387X},
	url = {https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/480},
	pages = {433--447}
}
@article{TACL652,
        author = {Michael Roth and Mirella Lapata},
        title = {Context-aware Frame-Semantic Role Labeling},
        journal = {Transactions of the Association for Computational Linguistics},
        volume = {3},
        year = {2015},
        keywords = {},
        abstract = {Frame semantic representations have been useful in several
applications ranging from text-to-scene generation, to question answering
and social network analysis. Predicting such representations from raw text
is, however, a challenging task and corresponding models are typically only
trained on a small set of sentence-level annotations. In this paper, we
present a semantic role labeling system that takes into account sentence and
discourse context. We introduce several new features which we motivate based
on linguistic insights and experimentally demonstrate that they lead to
significant improvements over the current state-of-the-art in FrameNet-based
semantic role labeling.
},
        issn = {2307-387X},
        url =
{https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/652},
        pages = {449--460}
}
@article{TACL584,
	author = {Daniel Beck and Trevor Cohn and Christian Hardmeier and Lucia Specia},
	title = {Learning Structural Kernels for Natural Language Processing},
	journal = {Transactions of the Association for Computational Linguistics},
	volume = {3},
	year = {2015},
	keywords = {},
	abstract = {Structural kernels are a flexible learning paradigm that has been widely used in Natural Language Processing. However, the problem of model selection in kernel-based methods is usually overlooked. Previous approaches mostly rely on setting default values for kernel hyperparameters or using grid search, which is slow and coarse-grained. In contrast, Bayesian methods allow efficient model selection by maximizing the evidence on the training data through gradient-based methods. In this paper we show how to perform this in the context of structural kernels by using Gaussian Processes. Experimental results on tree kernels show that this procedure results in better prediction performance compared to hyperparameter optimization via grid search. The framework proposed in this paper can be adapted to other structures besides trees, e.g., strings and graphs, thereby extending the utility of kernel-based methods.},
	issn = {2307-387X},
	url = {https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/584},
	pages = {461--473}
}

@article{TACL674,
	author = {Drew Reisinger and Rachel Rudinger and Francis Ferraro and Craig Harman and Kyle Rawlins and Benjamin Van Durme},
	title = {Semantic Proto-Roles},
	journal = {Transactions of the Association for Computational Linguistics},
	volume = {3},
	year = {2015},
	keywords = {},
	abstract = {We present the first large-scale, corpus based verification of the seminal work of Dowty on the notion of thematic proto-roles. Our results demonstrate both the need for and the feasibility of a property-based annotation scheme of semantic relationships, as opposed to the currently dominant notion of categorical roles.},
	issn = {2307-387X},
	url = {https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/674},
	pages = {475--488}
}
@article{TACL638,
	author = {Matthew Gormley and Mark Dredze and Jason Eisner},
	title = {Approximation-Aware Dependency Parsing by Belief Propagation},
	journal = {Transactions of the Association for Computational Linguistics},
	volume = {3},
	year = {2015},
	keywords = {},
	abstract = {We show how to train the fast dependency parser of Smith and Eisner (2008) for improved accuracy. This parser can consider higher-order interactions among edges while retaining O(n^3) runtime. It outputs the parse with maximum expected recall—but for speed, this expectation is taken under a posterior distribution that is constructed only approximately, using loopy belief propagation through structured factors. We show how to adjust the model parameters to compensate for the errors introduced by this approximation, by following the gradient of the actual loss on training data. We find this gradient by backpropagation. That is, we treat the entire parser (approximations and all) as a differentiable circuit, as others have done for loopy CRFs (Domke, 2010; Stoyanov et al., 2011; Domke, 2011; Stoyanov and Eisner, 2012). The resulting parser obtains higher accuracy with fewer iterations of belief propagation than one trained by conditional log-likelihood. },
	issn = {2307-387X},
	url = {https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/638},
	pages = {489--501}
}
@article{TACL637,
	author = {Nevena Lazic and Amarnag Subramanya and Michael Ringgaard and Fernando Pereira},
	title = {Plato: A Selective Context Model for Entity Resolution},
	journal = {Transactions of the Association for Computational Linguistics},
	volume = {3},
	year = {2015},
	keywords = {},
	abstract = {We present Plato, a probabilistic model for entity resolution that includes a novel approach for handling noisy or uninformative features, and supplements labeled training data derived from Wikipedia with a very large unlabeled text corpus. Training and inference in the proposed model can easily be distributed across many servers, allowing it to scale to over 10^7 entities. We evaluate Plato on three standard datasets for entity resolution. Our approach achieves the best results to-date on TAC KBP 2011 and is highly competitive on both the CoNLL 2003 and TAC KBP 2012 datasets.},
	issn = {2307-387X},
	url = {https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/637},
	pages = {503--515}
}
@article{TACL634,
        author = {Bishan Yang and Claire Cardie and Peter Frazier},
        title = {A Hierarchical Distance-dependent Bayesian Model for Event
Coreference Resolution},
        journal = {Transactions of the Association for Computational Linguistics},
        volume = {3},
        year = {2015},
        keywords = {},
        abstract = {We present a novel hierarchical distance-dependent Bayesian
model for event coreference resolution. While existing generative models for
event coreference resolution are completely unsupervised, our model allows
for the incorporation of pairwise distances between event mentions —
information that is widely used in supervised coreference models to guide
the generative clustering processing for better event clustering both within
and across documents. We model the distances between event mentions using a
feature-rich learnable distance function and encode them as Bayesian priors
for nonparametric clustering. Experiments on the ECB+ corpus show that our
model outperforms state-of-the-art methods for both within- and
cross-document event coreference resolution.},
        issn = {2307-387X},
        url =
{https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/634},
        pages = {517--528}
}
@article{TACL660,
        author = {Delli Bovi, Claudio  and Telesca, Luca  and Navigli, Roberto },
        title = {Large-Scale Information Extraction from Textual Definitions
through Deep Syntactic and Semantic Analysis},
        journal = {Transactions of the Association for Computational Linguistics},
        volume = {3},
        year = {2015},
        keywords = {},
        abstract = {We present DefIE, an approach to large-scale Information
Extraction (IE) based on a syntactic-semantic analysis of textual
definitions. Given a large corpus of definitions we leverage syntactic
dependencies to reduce data sparsity, then disambiguate the arguments and
content words of the relation strings, and finally exploit the resulting
information to organize the acquired relations hierarchically. The output of
DefIE is a high-quality knowledge base consisting of several million
automatically acquired semantic relations.},
        issn = {2307-387X},
        url =
{https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/660},
        pages = {529--543}
}
@article{TACL646,
        author = {Berant, Jonathan  and Liang, Percy },
        title = {Imitation Learning of Agenda-based Semantic Parsers},
        journal = {Transactions of the Association for Computational Linguistics},
        volume = {3},
        year = {2015},
        keywords = {},
        abstract = {Semantic parsers conventionally construct logical forms
bottom-up in a fixed order, resulting in the generation of many extraneous
partial logical forms. In this paper, we combine ideas from imitation
learning and agenda-based parsing to train a semantic parser that searches
partial logical forms in a more strategic order. Empirically, our parser
reduces the number of constructed partial logical forms by an order of
magnitude, and obtains a 6x-9x speedup over fixed-order parsing, while
maintaining comparable accuracy. },
        issn = {2307-387X},
        url =
{https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/646},
        pages = {545--558}
}
@article{TACL709,
	author = {Kuhlmann, Marco  and Jonsson, Peter },
	title = {Parsing to Noncrossing Dependency Graphs},
	journal = {Transactions of the Association for Computational Linguistics},
	volume = {3},
	year = {2015},
	keywords = {},
	abstract = {We study the generalization of maximum spanning tree dependency parsing to maximum acyclic subgraphs. Because the underlying optimization problem is intractable even under an arc-factored model, we consider the restriction to noncrossing dependency graphs. Our main contribution is a cubic-time exact inference algorithm for this class. We extend this algorithm into a practical parser and evaluate its performance on four linguistic data sets used in semantic dependency parsing. We also explore a generalization of our parsing framework to dependency graphs with pagenumber at most \$k\$ and show that the resulting optimization problem is NP-hard for \$k \geq 2\$.},
	issn = {2307-387X},
	url = {https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/709},
	pages = {559--570}
}
@article{TACL654,
	author = {Arthur, Philip  and Neubig, Graham  and Sakti, Sakriani  and Toda, Tomoki  and Nakamura, Satoshi },
	title = {Semantic Parsing of Ambiguous Input through Paraphrasing and Verification},
	journal = {Transactions of the Association for Computational Linguistics},
	volume = {3},
	year = {2015},
	keywords = {},
	abstract = {We propose a new method for semantic parsing of ambiguous and ungrammatical input, such as search queries. We do so by building on an existing semantic parsing framework that uses synchronous context free grammars (SCFG) to jointly model the input sentence and output meaning representation. We generalize this SCFG framework to allow not one, but multiple outputs. Using this formalism, we construct a grammar that takes an ambiguous input string and jointly maps it into both a meaning representation and a natural language paraphrase that is less ambiguous than the original input. This paraphrase can beused to disambiguate the meaning representation via verification using a language model that calculates the probability of each paraphrase.},
	issn = {2307-387X},
	url = {https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/654},
	pages = {571--584}
}
@article{TACL692,
        author = {Koncel-Kedziorski, Rik  and Hajishirzi, Hannaneh  and Sabharwal,
Ashish  and Etzioni, Oren  and Ang, Siena },
        title = {Parsing Algebraic Word Problems into Equations},
        journal = {Transactions of the Association for Computational Linguistics},
        volume = {3},
        year = {2015},
        keywords = {},
        abstract = {This paper formalizes the problem of solving multi-sentence
algebraic word problems as that of generating and scoring equation trees. We
use integer linear programming to generate equation trees and score their
likelihood by learning local and global discriminative models. These models
are trained on a small set of word problems and their answers, without any
manual annotation, in order to choose the equation that best matches the
problem text. We refer to the overall system as ALGES.  We compare ALGES
with previous work and show that it covers the full gamut of arithmetic
operations whereas Hosseini et al. (2014) only handle addition and
subtraction. In addition, ALGES overcomes the brittleness of the Kushman et
al. (2014) approach on single-equation problems, yielding a 15% to 50%
reduction in error.},
        issn = {2307-387X},
        url =
{https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/692},
        pages = {585--597}
}
