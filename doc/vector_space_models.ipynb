{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag-of-Words\n",
    "\n",
    "[CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'Natural Language Processing for Classification of Acute, Communicable Findings on Unstructured Head CT Reports: Comparison of Neural Network and Non-Neural Machine Learning Techniques', \n",
    "    'Lexicon Integrated CNN Models with Attention for Sentiment Analysis', \n",
    "    'Cross-domain Document Retrieval: Matching between Conversational and Formal Writings', \n",
    "    'Event Analysis on the 2016 U.S. Presidential Election Using Social Media',\n",
    "    'Improving Document Clustering by Eliminating Unnatural Language',\n",
    "    'Robust Coreference Resolution and Entity Linking on Dialogues: Character Identification on TV Show Transcripts',\n",
    "    'Text-based Speaker Identification on Multiparty Dialogues Using Multi-document Convolutional Neural Networks',\n",
    "    'Classification of Radiology Reports Using Neural Attention Models',\n",
    "    'Deep Dependency Graph Conversion in English']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vsm_1gram = CountVectorizer()\n",
    "X1 = vsm_1gram.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0: 2016\n",
      " 1: acute\n",
      " 2: analysis\n",
      " 3: and\n",
      " 4: attention\n",
      " 5: based\n",
      " 6: between\n",
      " 7: by\n",
      " 8: character\n",
      " 9: classification\n",
      "10: clustering\n",
      "11: cnn\n",
      "12: communicable\n",
      "13: comparison\n",
      "14: conversational\n",
      "15: conversion\n",
      "16: convolutional\n",
      "17: coreference\n",
      "18: cross\n",
      "19: ct\n",
      "20: deep\n",
      "21: dependency\n",
      "22: dialogues\n",
      "23: document\n",
      "24: domain\n",
      "25: election\n",
      "26: eliminating\n",
      "27: english\n",
      "28: entity\n",
      "29: event\n",
      "30: findings\n",
      "31: for\n",
      "32: formal\n",
      "33: graph\n",
      "34: head\n",
      "35: identification\n",
      "36: improving\n",
      "37: in\n",
      "38: integrated\n",
      "39: language\n",
      "40: learning\n",
      "41: lexicon\n",
      "42: linking\n",
      "43: machine\n",
      "44: matching\n",
      "45: media\n",
      "46: models\n",
      "47: multi\n",
      "48: multiparty\n",
      "49: natural\n",
      "50: network\n",
      "51: networks\n",
      "52: neural\n",
      "53: non\n",
      "54: of\n",
      "55: on\n",
      "56: presidential\n",
      "57: processing\n",
      "58: radiology\n",
      "59: reports\n",
      "60: resolution\n",
      "61: retrieval\n",
      "62: robust\n",
      "63: sentiment\n",
      "64: show\n",
      "65: social\n",
      "66: speaker\n",
      "67: techniques\n",
      "68: text\n",
      "69: the\n",
      "70: transcripts\n",
      "71: tv\n",
      "72: unnatural\n",
      "73: unstructured\n",
      "74: using\n",
      "75: with\n",
      "76: writings\n"
     ]
    }
   ],
   "source": [
    "for i, title in enumerate(vsm_1gram.get_feature_names()):\n",
    "    print('%2d: %s' % (i, title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 67)\t1\n",
      "  (0, 40)\t1\n",
      "  (0, 43)\t1\n",
      "  (0, 53)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 50)\t1\n",
      "  (0, 52)\t2\n",
      "  (0, 13)\t1\n",
      "  (0, 59)\t1\n",
      "  (0, 19)\t1\n",
      "  (0, 34)\t1\n",
      "  (0, 73)\t1\n",
      "  (0, 55)\t1\n",
      "  (0, 30)\t1\n",
      "  (0, 12)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 54)\t2\n",
      "  (0, 9)\t1\n",
      "  (0, 31)\t1\n",
      "  (0, 57)\t1\n",
      "  (0, 39)\t1\n",
      "  (0, 49)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 63)\t1\n",
      "  (1, 4)\t1\n",
      "  :\t:\n",
      "  (6, 47)\t1\n",
      "  (6, 48)\t1\n",
      "  (6, 66)\t1\n",
      "  (6, 5)\t1\n",
      "  (6, 68)\t1\n",
      "  (6, 35)\t1\n",
      "  (6, 22)\t1\n",
      "  (6, 74)\t1\n",
      "  (6, 23)\t1\n",
      "  (6, 52)\t1\n",
      "  (6, 55)\t1\n",
      "  (7, 58)\t1\n",
      "  (7, 74)\t1\n",
      "  (7, 4)\t1\n",
      "  (7, 46)\t1\n",
      "  (7, 52)\t1\n",
      "  (7, 59)\t1\n",
      "  (7, 54)\t1\n",
      "  (7, 9)\t1\n",
      "  (8, 27)\t1\n",
      "  (8, 37)\t1\n",
      "  (8, 15)\t1\n",
      "  (8, 33)\t1\n",
      "  (8, 21)\t1\n",
      "  (8, 20)\t1\n"
     ]
    }
   ],
   "source": [
    "print(X1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vsm_2gram = CountVectorizer(ngram_range=(1, 2))\n",
    "X = vsm_2gram.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0: 2016\n",
      " 1: 2016 presidential\n",
      " 2: acute\n",
      " 3: acute communicable\n",
      " 4: analysis\n",
      " 5: analysis on\n",
      " 6: and\n",
      " 7: and entity\n",
      " 8: and formal\n",
      " 9: and non\n",
      "10: attention\n",
      "11: attention for\n",
      "12: attention models\n",
      "13: based\n",
      "14: based speaker\n",
      "15: between\n",
      "16: between conversational\n",
      "17: by\n",
      "18: by eliminating\n",
      "19: character\n",
      "20: character identification\n",
      "21: classification\n",
      "22: classification of\n",
      "23: clustering\n",
      "24: clustering by\n",
      "25: cnn\n",
      "26: cnn models\n",
      "27: communicable\n",
      "28: communicable findings\n",
      "29: comparison\n",
      "30: comparison of\n",
      "31: conversational\n",
      "32: conversational and\n",
      "33: conversion\n",
      "34: conversion in\n",
      "35: convolutional\n",
      "36: convolutional neural\n",
      "37: coreference\n",
      "38: coreference resolution\n",
      "39: cross\n",
      "40: cross domain\n",
      "41: ct\n",
      "42: ct reports\n",
      "43: deep\n",
      "44: deep dependency\n",
      "45: dependency\n",
      "46: dependency graph\n",
      "47: dialogues\n",
      "48: dialogues character\n",
      "49: dialogues using\n",
      "50: document\n",
      "51: document clustering\n",
      "52: document convolutional\n",
      "53: document retrieval\n",
      "54: domain\n",
      "55: domain document\n",
      "56: election\n",
      "57: election using\n",
      "58: eliminating\n",
      "59: eliminating unnatural\n",
      "60: english\n",
      "61: entity\n",
      "62: entity linking\n",
      "63: event\n",
      "64: event analysis\n",
      "65: findings\n",
      "66: findings on\n",
      "67: for\n",
      "68: for classification\n",
      "69: for sentiment\n",
      "70: formal\n",
      "71: formal writings\n",
      "72: graph\n",
      "73: graph conversion\n",
      "74: head\n",
      "75: head ct\n",
      "76: identification\n",
      "77: identification on\n",
      "78: improving\n",
      "79: improving document\n",
      "80: in\n",
      "81: in english\n",
      "82: integrated\n",
      "83: integrated cnn\n",
      "84: language\n",
      "85: language processing\n",
      "86: learning\n",
      "87: learning techniques\n",
      "88: lexicon\n",
      "89: lexicon integrated\n",
      "90: linking\n",
      "91: linking on\n",
      "92: machine\n",
      "93: machine learning\n",
      "94: matching\n",
      "95: matching between\n",
      "96: media\n",
      "97: models\n",
      "98: models with\n",
      "99: multi\n",
      "100: multi document\n",
      "101: multiparty\n",
      "102: multiparty dialogues\n",
      "103: natural\n",
      "104: natural language\n",
      "105: network\n",
      "106: network and\n",
      "107: networks\n",
      "108: neural\n",
      "109: neural attention\n",
      "110: neural machine\n",
      "111: neural network\n",
      "112: neural networks\n",
      "113: non\n",
      "114: non neural\n",
      "115: of\n",
      "116: of acute\n",
      "117: of neural\n",
      "118: of radiology\n",
      "119: on\n",
      "120: on dialogues\n",
      "121: on multiparty\n",
      "122: on the\n",
      "123: on tv\n",
      "124: on unstructured\n",
      "125: presidential\n",
      "126: presidential election\n",
      "127: processing\n",
      "128: processing for\n",
      "129: radiology\n",
      "130: radiology reports\n",
      "131: reports\n",
      "132: reports comparison\n",
      "133: reports using\n",
      "134: resolution\n",
      "135: resolution and\n",
      "136: retrieval\n",
      "137: retrieval matching\n",
      "138: robust\n",
      "139: robust coreference\n",
      "140: sentiment\n",
      "141: sentiment analysis\n",
      "142: show\n",
      "143: show transcripts\n",
      "144: social\n",
      "145: social media\n",
      "146: speaker\n",
      "147: speaker identification\n",
      "148: techniques\n",
      "149: text\n",
      "150: text based\n",
      "151: the\n",
      "152: the 2016\n",
      "153: transcripts\n",
      "154: tv\n",
      "155: tv show\n",
      "156: unnatural\n",
      "157: unnatural language\n",
      "158: unstructured\n",
      "159: unstructured head\n",
      "160: using\n",
      "161: using multi\n",
      "162: using neural\n",
      "163: using social\n",
      "164: with\n",
      "165: with attention\n",
      "166: writings\n"
     ]
    }
   ],
   "source": [
    "for i, title in enumerate(vsm_2gram.get_feature_names()):\n",
    "    print('%2d: %s' % (i, title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 87)\t1\n",
      "  (0, 93)\t1\n",
      "  (0, 110)\t1\n",
      "  (0, 114)\t1\n",
      "  (0, 9)\t1\n",
      "  (0, 106)\t1\n",
      "  (0, 111)\t1\n",
      "  (0, 117)\t1\n",
      "  (0, 30)\t1\n",
      "  (0, 132)\t1\n",
      "  (0, 42)\t1\n",
      "  (0, 75)\t1\n",
      "  (0, 159)\t1\n",
      "  (0, 124)\t1\n",
      "  (0, 66)\t1\n",
      "  (0, 28)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 116)\t1\n",
      "  (0, 22)\t1\n",
      "  (0, 68)\t1\n",
      "  (0, 128)\t1\n",
      "  (0, 85)\t1\n",
      "  (0, 104)\t1\n",
      "  (0, 148)\t1\n",
      "  (0, 86)\t1\n",
      "  :\t:\n",
      "  (7, 109)\t1\n",
      "  (7, 162)\t1\n",
      "  (7, 133)\t1\n",
      "  (7, 130)\t1\n",
      "  (7, 118)\t1\n",
      "  (7, 129)\t1\n",
      "  (7, 160)\t1\n",
      "  (7, 10)\t1\n",
      "  (7, 97)\t1\n",
      "  (7, 22)\t1\n",
      "  (7, 108)\t1\n",
      "  (7, 131)\t1\n",
      "  (7, 115)\t1\n",
      "  (7, 21)\t1\n",
      "  (8, 81)\t1\n",
      "  (8, 34)\t1\n",
      "  (8, 73)\t1\n",
      "  (8, 46)\t1\n",
      "  (8, 44)\t1\n",
      "  (8, 60)\t1\n",
      "  (8, 80)\t1\n",
      "  (8, 33)\t1\n",
      "  (8, 72)\t1\n",
      "  (8, 45)\t1\n",
      "  (8, 43)\t1\n"
     ]
    }
   ],
   "source": [
    "X2 = vsm_2gram.fit_transform(corpus)\n",
    "print(X2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "\n",
    "[TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vsm_tfidf = TfidfVectorizer()\n",
    "X3 = vsm_tfidf.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 49)\t0.209314469882\n",
      "  (0, 39)\t0.176790333648\n",
      "  (0, 57)\t0.209314469882\n",
      "  (0, 31)\t0.176790333648\n",
      "  (0, 9)\t0.176790333648\n",
      "  (0, 54)\t0.353580667296\n",
      "  (0, 1)\t0.209314469882\n",
      "  (0, 12)\t0.209314469882\n",
      "  (0, 30)\t0.209314469882\n",
      "  (0, 55)\t0.13581476794\n",
      "  (0, 73)\t0.209314469882\n",
      "  (0, 34)\t0.209314469882\n",
      "  (0, 19)\t0.209314469882\n",
      "  (0, 59)\t0.176790333648\n",
      "  (0, 13)\t0.209314469882\n",
      "  (0, 52)\t0.307428183496\n",
      "  (0, 50)\t0.209314469882\n",
      "  (0, 3)\t0.153714091748\n",
      "  (0, 53)\t0.209314469882\n",
      "  (0, 43)\t0.209314469882\n",
      "  (0, 40)\t0.209314469882\n",
      "  (0, 67)\t0.209314469882\n",
      "  (1, 31)\t0.301389086958\n",
      "  (1, 41)\t0.356835668915\n",
      "  (1, 38)\t0.356835668915\n",
      "  :\t:\n",
      "  (6, 23)\t0.227002758778\n",
      "  (6, 74)\t0.227002758778\n",
      "  (6, 22)\t0.261081420753\n",
      "  (6, 35)\t0.261081420753\n",
      "  (6, 68)\t0.309112597128\n",
      "  (6, 5)\t0.309112597128\n",
      "  (6, 66)\t0.309112597128\n",
      "  (6, 48)\t0.309112597128\n",
      "  (6, 47)\t0.309112597128\n",
      "  (6, 16)\t0.309112597128\n",
      "  (6, 51)\t0.309112597128\n",
      "  (7, 9)\t0.355474920167\n",
      "  (7, 54)\t0.355474920167\n",
      "  (7, 59)\t0.355474920167\n",
      "  (7, 52)\t0.309075181687\n",
      "  (7, 46)\t0.355474920167\n",
      "  (7, 4)\t0.355474920167\n",
      "  (7, 74)\t0.309075181687\n",
      "  (7, 58)\t0.420871678535\n",
      "  (8, 20)\t0.408248290464\n",
      "  (8, 21)\t0.408248290464\n",
      "  (8, 33)\t0.408248290464\n",
      "  (8, 15)\t0.408248290464\n",
      "  (8, 37)\t0.408248290464\n",
      "  (8, 27)\t0.408248290464\n"
     ]
    }
   ],
   "source": [
    "print(X3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "[Clustering Algorithms](http://scikit-learn.org/stable/modules/clustering.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "Y1_kmeans = KMeans(n_clusters=2, random_state=0).fit_predict(X1)\n",
    "Y2_kmeans = KMeans(n_clusters=2, random_state=0).fit_predict(X2)\n",
    "Y3_kmeans = KMeans(n_clusters=2, random_state=0).fit_predict(X3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 0 0 0 0 0 0]\n",
      "[1 0 0 0 0 0 0 0 0]\n",
      "[1 1 0 0 0 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(Y1_kmeans)\n",
    "print(Y2_kmeans)\n",
    "print(Y3_kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "Y1_spectral = SpectralClustering(n_clusters=2, random_state=0).fit_predict(X1)\n",
    "Y2_spectral = SpectralClustering(n_clusters=2, random_state=0).fit_predict(X2)\n",
    "Y3_spectral = SpectralClustering(n_clusters=2, random_state=0).fit_predict(X3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 0 0 0 1 0]\n",
      "[0 0 0 0 1 0 0 0 1]\n",
      "[0 0 1 1 1 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(Y1_spectral)\n",
    "print(Y2_spectral)\n",
    "print(Y3_spectral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
